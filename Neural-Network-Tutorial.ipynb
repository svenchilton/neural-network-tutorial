{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Artificial neural networks comprise a broad class of computational algorithms based loosely on an older understanding of neuroscience in the human brain.  In recent years, deep (i.e. many-layered) neural networks have formed the foundation of deep learning, a subset of machine learning characterized by the ability to learn features of data which in prior computational eras required explicit human encoding.  \n",
    "\n",
    "All neural networks are based on _neurons_ (or analogous structures): \n",
    "\n",
    "<img src=\"images/Neuron.jpg\">\n",
    "\n",
    "Image taken from [StackExchange](https://tex.stackexchange.com/questions/132444/diagram-of-an-artificial-neural-network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each neuron takes a _weighted_ sum of its inputs (the components of a vector, or the column entries of a particular row in a spreadsheet), adds an offset or _bias_ term, and applies a nonlinear _activation function._  Some of the more common activation functions are illustrated below: \n",
    "\n",
    "<img src='images/Common-Activation-Functions.png'>\n",
    "\n",
    "Image taken from [Machine Learning World](https://medium.com/machine-learning-world/how-to-debug-neural-networks-manual-dc2a200f10f2) blog post on [medium.com](https://medium.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _layer_ of neurons consists of multiple neurons acting on the same inputs with different weights and biases.  The first hidden layer acts on the original data.  The subsequent layers, in turn, take the activations from the previous layer as inputs.  The activations of the final (output) layer (often combined with some sort of decision function) tell us what we wish to know from the original data.  An architecture such as the one illustrated below is termed a _fully connected neural network._\n",
    "\n",
    "<img src=\"images/Hidden-Layer.jpg\">\n",
    "\n",
    "Image taken from [python3.codes](http://python3.codes/neural-network-python-part-1-sigmoid-function-gradient-descent-backpropagation/) tutorial on neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To familiarize yourself with the consequences of different choices of problem, features, and (hyper)parameters, I recommend exploring the [Tensorflow Playground](https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.51041&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=true&xSquared=true&ySquared=true&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false), from which I've included the screenshot below: \n",
    "\n",
    "<img src=\"images/Tensorflow-Playground.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll likely find attempting to choose the proper weights and biases by hand tremendously difficult.  In practice, one typically learns the weights through a numerical optimization scheme.  One first defines a cost function to quantify the difference between the actual and predicted neural network outputs and then uses gradients of that function to find the optimum weights and biases.  The schematic below illustrates and contrasts three common approaches to this optimization:  \n",
    "\n",
    "<img src='images/Gradient-Descent.png'>\n",
    "\n",
    "Image taken from [Towards Data Science](https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3) blog post, presumably adapted from [Coursera Deep Learning](https://www.coursera.org/specializations/deep-learning) course on [Improving Deep Neural Networks](https://www.coursera.org/learn/deep-neural-network/home/welcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now consider image data.  A digital image is typically represented as a 3D array, where the dimensions are pixel height, pixel width, and channel number (1 for B/W images, 3 for RGB color images).  Each element in such an array is a pixel intensity.  One could rearrange these elements into a 1D array and feed that array into a fully connected neural network, but that would be prohibitively costly.  As such, _convolutional neural networks_ were introduced.  The base unit of a CNN, a _filter_, is distinguished from a neuron only in the arrangement of its input data, as the examples below illustrate: \n",
    "\n",
    "<img src=\"images/Convolution_schematic.gif\" style=\"width:500px;height:300px;\">\n",
    "\n",
    "GIF taken from [Coursera Deep Learning](https://www.coursera.org/specializations/deep-learning) course on [Convolutional Neural Networks](https://www.coursera.org/learn/convolutional-neural-networks/home/welcome)\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/conv_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "Video taken from [Coursera Deep Learning](https://www.coursera.org/specializations/deep-learning) course on [Convolutional Neural Networks](https://www.coursera.org/learn/convolutional-neural-networks/home/welcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To maximize the contribution of elements near the edge of the input volume to the output volume, and in some cases to ensure that the output volume has the same height and width as the input volume, one typically _pads_ the input volume with zeros: \n",
    "\n",
    "<img src=\"images/PAD.png\" style=\"width:600px;height:400px;\">\n",
    "\n",
    "Image taken from [Coursera Deep Learning](https://www.coursera.org/specializations/deep-learning) course on [Convolutional Neural Networks](https://www.coursera.org/learn/convolutional-neural-networks/home/welcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A filter's and input volume's channel numbers must match.  However, a convolutional layer typically consists of multiple filters.  The number of filters becomes the channel number of the output volume.\n",
    "\n",
    "<img src=\"images/conv_nn.png\" style=\"width:350px;height:200px;\">\n",
    "\n",
    "Image taken from [Coursera Deep Learning](https://www.coursera.org/specializations/deep-learning) course on [Convolutional Neural Networks](https://www.coursera.org/learn/convolutional-neural-networks/home/welcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling layers reduce the height and width of the input. They help reduce computation, and make feature detectors more invariant to its position in the input. The two types of pooling layers are: \n",
    "\n",
    "- Max-pooling layer: slides an ($f, f$) window over the input and stores the max value of the window in the output.\n",
    "\n",
    "- Average-pooling layer: slides an ($f, f$) window over the input and stores the average value of the window in the output.\n",
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/max_pool1.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "\n",
    "<td>\n",
    "<img src=\"images/a_pool.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "</table>\n",
    "\n",
    "Images taken from [Coursera Deep Learning](https://www.coursera.org/specializations/deep-learning) course on [Convolutional Neural Networks](https://www.coursera.org/learn/convolutional-neural-networks/home/welcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic ConvNet architecture might look something like this: \n",
    "\n",
    "<img src=\"images/basic-cnn-architecture.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "Image taken from [Coursera Deep Learning](https://www.coursera.org/specializations/deep-learning) course on [Convolutional Neural Networks](https://www.coursera.org/learn/convolutional-neural-networks/home/welcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilayered ConvNets can encounter vanishing gradient problems, resulting in longer training times and less accurate models.  To combat this, ResNets (_residual networks_) were introduced.  In ResNets, data from earlier layers is added to data from newer layers, which helps ensure that the weights and biases in the earlier layers are updated properly during training.  This typically takes place in either an _identity block_, where the data from the earlier layer is added directly to the data from the later layer, \n",
    "\n",
    "<img src=\"images/idblock3_kiank.png\" style=\"width:650px;height:150px;\">\n",
    "\n",
    "or a _convolutional block_, where the data from the earlier layer is convolved before adding it to the data from the later layer.\n",
    "\n",
    "<img src=\"images/convblock_kiank.png\" style=\"width:650px;height:150px;\">\n",
    "\n",
    "Images taken from [Coursera Deep Learning](https://www.coursera.org/specializations/deep-learning) course on [Convolutional Neural Networks](https://www.coursera.org/learn/convolutional-neural-networks/home/welcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, ResNet50 (typically used for image classification problems) has the architecture below: \n",
    "\n",
    "<img src=\"images/resnet_kiank.png\" style=\"width:850px;height:150px;\">\n",
    "\n",
    "Image taken from [Coursera Deep Learning](https://www.coursera.org/specializations/deep-learning) course on [Convolutional Neural Networks](https://www.coursera.org/learn/convolutional-neural-networks/home/welcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text-based problems typically call for _sequence models_.  The base unit of a sequence model is a _cell_, which differs from a neuron in that it outputs an array rather than a single number.  Moreover, the activations from one cell serve as an input to the next cell, along with the next training example.  \n",
    "\n",
    "<img src=\"images/RNN.png\" style=\"width:500;height:300px;\">\n",
    "\n",
    "Image taken from [Coursera Deep Learning](https://www.coursera.org/specializations/deep-learning) course on [Sequence Models](https://www.coursera.org/learn/sequence-models/home/welcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Recurrent neural networks_ (RNNs) are the archetypal sequence model.  An RNN cell has the following structure: \n",
    "\n",
    "<img src=\"images/rnn_step_forward.png\" style=\"width:700px;height:300px;\">\n",
    "<caption><center> Basic RNN cell. Takes as input $x^{\\langle t \\rangle}$ (current input) and $a^{\\langle t - 1\\rangle}$ (previous hidden state containing information from the past), and outputs $a^{\\langle t \\rangle}$ which is given to the next RNN cell and also used to predict $y^{\\langle t \\rangle}$ </center></caption>\n",
    "\n",
    "Image taken from [Coursera Deep Learning](https://www.coursera.org/specializations/deep-learning) course on [Sequence Models](https://www.coursera.org/learn/sequence-models/home/welcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical many-to-many RNN (in this case, where the output has as many time steps as the input) might have the following architecture: \n",
    "\n",
    "<img src=\"images/rnn.png\" style=\"width:800px;height:300px;\">\n",
    "<caption><center> Basic RNN. The input sequence $x = (x^{\\langle 1 \\rangle}, x^{\\langle 2 \\rangle}, ..., x^{\\langle T_x \\rangle})$  is carried over $T_x$ time steps. The network outputs $y = (y^{\\langle 1 \\rangle}, y^{\\langle 2 \\rangle}, ..., y^{\\langle T_x \\rangle})$. </center></caption>\n",
    "\n",
    "Image taken from [Coursera Deep Learning](https://www.coursera.org/specializations/deep-learning) course on [Sequence Models](https://www.coursera.org/learn/sequence-models/home/welcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _long short term memory_ (LSTM) network is a variant of the RNN.  In addition to the activation $a^{\\langle t \\rangle}$, it tracks and updates a \"cell state\" or memory variable $c^{\\langle t \\rangle}$ at every time step.  An LSTM cell is structured as illustrated below.  Roughly speaking, the weights in the forget and update gates determine how much of the input data at the current time step and activation from the previous time step should be used to forget previous state information and update the current state accordingly.\n",
    "\n",
    "<img src=\"images/LSTM.png\" style=\"width:500;height:400px;\">\n",
    "\n",
    "Image taken from [Coursera Deep Learning](https://www.coursera.org/specializations/deep-learning) course on [Sequence Models](https://www.coursera.org/learn/sequence-models/home/welcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM over multiple time-steps: \n",
    "\n",
    "<img src=\"images/LSTM_rnn.png\" style=\"width:500;height:300px;\">\n",
    "\n",
    "Image taken from [Coursera Deep Learning](https://www.coursera.org/specializations/deep-learning) course on [Sequence Models](https://www.coursera.org/learn/sequence-models/home/welcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can string together and stack RNN or LSTM cells in many more ways.  For example, the network architecture below could be used to make character-level predictions (such as when you're writing a text message):\n",
    "\n",
    "<img src=\"images/dinos3.png\" style=\"width:500;height:300px;\">\n",
    "\n",
    "The architecture below could be used for a sentiment classification problem (such as an Amazon or Yelp review):\n",
    "\n",
    "<img src=\"images/emojifier-v2.png\" style=\"width:700px;height:400px;\">\n",
    "\n",
    "While a machine translation problem might employ an architecture like the one below, with a bi-directional LSTM and an attention model: \n",
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/attn_model.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td>\n",
    "<td>\n",
    "<img src=\"images/attn_mechanism.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td>\n",
    "</table>\n",
    "\n",
    "Images taken from [Coursera Deep Learning](https://www.coursera.org/specializations/deep-learning) course on [Sequence Models](https://www.coursera.org/learn/sequence-models/home/welcome)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
